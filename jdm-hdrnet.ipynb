{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"mount_file_id":"157nuVYQ2XQkOgKN6VrUmo_WJsc6K0h0Z","authorship_tag":"ABX9TyNpvGrtUTe5AwuRga7+ODnJ"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9647424,"sourceType":"datasetVersion","datasetId":5892023},{"sourceId":141972,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":120270,"modelId":143486},{"sourceId":143373,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":121472,"modelId":144617},{"sourceId":143376,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":121475,"modelId":144620}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch\n!pip install torchvision\n!pip install tensorboard\n!pip install numpy\n!pip install matplotlib","metadata":{"id":"-PNw0-BM6AsG","executionInfo":{"status":"ok","timestamp":1729130695823,"user_tz":-330,"elapsed":38213,"user":{"displayName":"Rohan Phad","userId":"04082063373055029584"}},"outputId":"3badda66-61e9-4417-b44b-4cbf759c9dd8","execution":{"iopub.status.busy":"2024-10-22T15:17:03.332113Z","iopub.execute_input":"2024-10-22T15:17:03.332796Z","iopub.status.idle":"2024-10-22T15:18:00.752150Z","shell.execute_reply.started":"2024-10-22T15:17:03.332755Z","shell.execute_reply":"2024-10-22T15:18:00.751025Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.4.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->torchvision) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->torchvision) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->torchvision) (1.3.0)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.16.2)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.62.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.6)\nRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.26.4)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (70.0.0)\nRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.0.4)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: numpy<2,>=1.20 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install imagecodecs\n!pip install einops","metadata":{"id":"xr1hhZRsMtyw","executionInfo":{"status":"ok","timestamp":1729130706701,"user_tz":-330,"elapsed":10891,"user":{"displayName":"Rohan Phad","userId":"04082063373055029584"}},"outputId":"04edeb7b-5e88-4a67-f4f2-249077dd015c","execution":{"iopub.status.busy":"2024-10-22T15:18:00.754219Z","iopub.execute_input":"2024-10-22T15:18:00.754531Z","iopub.status.idle":"2024-10-22T15:18:25.650576Z","shell.execute_reply.started":"2024-10-22T15:18:00.754497Z","shell.execute_reply":"2024-10-22T15:18:25.649611Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Collecting imagecodecs\n  Downloading imagecodecs-2024.9.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from imagecodecs) (1.26.4)\nDownloading imagecodecs-2024.9.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: imagecodecs\nSuccessfully installed imagecodecs-2024.9.22\nCollecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.8.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install opencv-python\n!pip install scikit-image","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:12:36.822761Z","iopub.execute_input":"2024-10-22T16:12:36.823619Z","iopub.status.idle":"2024-10-22T16:12:59.611904Z","shell.execute_reply.started":"2024-10-22T16:12:36.823579Z","shell.execute_reply":"2024-10-22T16:12:59.610663Z"},"trusted":true},"execution_count":110,"outputs":[{"name":"stdout","text":"Requirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (4.10.0.84)\nRequirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from opencv-python) (1.26.4)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (0.23.2)\nRequirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (1.26.4)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (1.14.1)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (3.3)\nRequirement already satisfied: pillow>=9.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (10.3.0)\nRequirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2.34.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2024.5.22)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (21.3)\nRequirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (0.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image) (3.1.2)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# HDRNet","metadata":{"id":"DAYrHk7t9dTI"}},{"cell_type":"markdown","source":"##Utils","metadata":{"id":"cQTTJkop9T_D"}},{"cell_type":"code","source":"import json\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport re\nimport torch\nimport torch.nn.functional as F\nfrom matplotlib.ticker import MaxNLocator\nimport cv2\nfrom skimage.metrics import structural_similarity as ssim\n\n\n\ndef psnr(pred, target):\n    return 10 * torch.log10(1 / F.mse_loss(pred, target))\n\ndef calculate_ssim(image1, image2):\n    # Convert to grayscale if the images are colored\n    if len(image1.shape) == 3:\n        image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n    if len(image2.shape) == 3:\n        image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n\n    ssim_value, _ = ssim(image1, image2, full=True)\n    return ssim_value\n\n\ndef print_params(params):\n    print('Training parameters: ')\n    print('\\n'.join('  {} = {}'.format(k, str(v)) for k, v in params.items()))\n    print()\n\ndef get_files(path):\n    files = os.listdir(path)\n    files = [os.path.join(path, x) for x in files]\n    files.sort()\n    return files\n\ndef load_train_ckpt(model, ckpt_dir):\n    # Get latest\n    files = os.listdir(ckpt_dir)\n    if not files:\n        return\n    files = [os.path.join(ckpt_dir, x) for x in files]\n    files.sort(key=lambda f: int(re.sub('\\D', '', f)))\n    ckpt_path = files[-1]\n    prev_epochs = -1\n    prev_epochs = int(ckpt_path.split('_')[1])\n    print(\"epochs \", prev_epochs)\n    # Load ckpt\n    print('Loading:', ckpt_path)\n    state_dict = torch.load(ckpt_path)\n    state_dict.pop('params')\n    model.load_state_dict(state_dict)\n    return prev_epochs\n\ndef load_test_ckpt(ckpt_path):\n    state_dict = torch.load(ckpt_path)\n    params = state_dict['params']\n    state_dict.pop('params')\n    return state_dict, params\n\ndef save_model_stats(model, params, ckpt_fname, stats):\n    ckpt_path = os.path.join(params['ckpt_dir'], ckpt_fname)\n    state_dict = model.state_dict()\n    state_dict['params'] = params\n    torch.save(state_dict, ckpt_path)\n    # Save stats\n    stats_path = os.path.join(params['stats_dir'], 'stats.json')\n    with open(stats_path, 'w') as fp:\n        json.dump(stats, fp, indent=2)\n\n\nclass AvgMeter(object):\n    \"\"\"Acumulate and compute average.\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0.\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"id":"uWqW_Ks09TPx","executionInfo":{"status":"ok","timestamp":1729141900567,"user_tz":-330,"elapsed":2430,"user":{"displayName":"Rohan Phad","userId":"04082063373055029584"}},"execution":{"iopub.status.busy":"2024-10-22T16:29:30.217954Z","iopub.execute_input":"2024-10-22T16:29:30.218765Z","iopub.status.idle":"2024-10-22T16:29:30.235971Z","shell.execute_reply.started":"2024-10-22T16:29:30.218722Z","shell.execute_reply":"2024-10-22T16:29:30.234971Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"markdown","source":"## Layers","metadata":{"id":"U_mLJiG6-rbY"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.utils import save_image\n\ndef conv_layer(in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True, activation=nn.ReLU, batch_norm=False):\n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)]\n    if batch_norm:\n        layers.append(nn.BatchNorm2d(out_channels))\n    if activation:\n        layers.append(activation())\n    return nn.Sequential(*layers)\n\ndef fc_layer(in_channels, out_channels, bias=True, activation=nn.ReLU, batch_norm=False):\n    layers = [nn.Linear(int(in_channels), int(out_channels), bias=bias)]\n    if batch_norm:\n        layers.append(nn.BatchNorm1d(out_channels))\n    if activation:\n        layers.append(activation())\n    return nn.Sequential(*layers)\n\ndef slicing(grid, guide):#grid N, C=12, D=8, H=16, W=16  # guide N, C=1, H, W\n    N, C, H, W = guide.shape\n    device = grid.get_device()\n    if device >= 0:\n        hh, ww = torch.meshgrid(torch.arange(H, device=device), torch.arange(W, device=device)) # H, W\n    else:\n        hh, ww = torch.meshgrid(torch.arange(H), torch.arange(W)) # H, W\n    # To [-1, 1] range for grid_sample\n    hh = hh / (H - 1) * 2 - 1\n    ww = ww / (W - 1) * 2 - 1\n    guide = guide * 2 - 1\n    hh = hh[None, :, :, None].repeat(N, 1, 1, 1) # N, H, W, C=1\n    ww = ww[None, :, :, None].repeat(N, 1, 1, 1)  # N, H, W, C=1\n    guide = guide.permute(0, 2, 3, 1) # N, H, W, C=1\n\n    guide_coords = torch.cat([ww, hh, guide], dim=3) # N, H, W, 3    guide-> D channel\n    # unsqueeze because extra D dimension\n    guide_coords = guide_coords.unsqueeze(1) # N, Dout=1, H, W, 3 # H W->final size\n    sliced = F.grid_sample(grid, guide_coords, align_corners=False, padding_mode=\"border\") # N, C=12, Dout=1, H, W\n    sliced = sliced.squeeze(2) # N, C=12, H, W\n\n    return sliced\n\ndef apply(sliced, fullres):\n    # r' = w1*r + w2*g + w3*b + w4\n    rr = fullres * sliced[:, 0:3, :, :] # N, C=3, H, W\n    gg = fullres * sliced[:, 4:7, :, :] # N, C=3, H, W\n    bb = fullres * sliced[:, 8:11, :, :] # N, C=3, H, W\n    rr = torch.sum(rr, dim=1) + sliced[:, 3, :, :] # N, H, W\n    gg = torch.sum(gg, dim=1) + sliced[:, 7, :, :] # N, H, W\n    bb = torch.sum(bb, dim=1) + sliced[:, 11, :, :] # N, H, W\n    output = torch.stack([rr, gg, bb], dim=1) # N, C=3, H, W\n    return output","metadata":{"id":"wnABdzEk-rEc","executionInfo":{"status":"ok","timestamp":1729141901905,"user_tz":-330,"elapsed":1348,"user":{"displayName":"Rohan Phad","userId":"04082063373055029584"}},"execution":{"iopub.status.busy":"2024-10-22T16:12:59.637690Z","iopub.execute_input":"2024-10-22T16:12:59.638485Z","iopub.status.idle":"2024-10-22T16:12:59.655734Z","shell.execute_reply.started":"2024-10-22T16:12:59.638429Z","shell.execute_reply":"2024-10-22T16:12:59.654918Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"markdown","source":"## Modules","metadata":{"id":"-OcQ7PLT-1OZ"}},{"cell_type":"code","source":"import torch.nn.functional as F\nfrom PIL import Image\nfrom torchvision.transforms.functional import resize\nfrom einops import rearrange\n\nclass SPSA_Attention(nn.Module):\n    def __init__(self, dim, num_heads,is_material_mask, is_spec, bias):\n        super(SPSA_Attention, self).__init__()\n        self.num_heads = num_heads\n        self.is_material_mask = is_material_mask\n        self.is_spec = is_spec\n        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n\n        self.qkv = nn.Conv2d(dim, dim*3, kernel_size=1, bias=bias)\n        self.qkv_dwconv = nn.Conv2d(dim*3, dim*3, kernel_size=3, stride=1, padding=1, groups=dim*3, bias=bias)\n        if self.is_spec:\n            self.project_out1_x = nn.Conv2d(dim//2, dim//2,  kernel_size=3, stride=1, padding=1, bias=bias)\n            self.project_out1_spec = nn.Conv2d(dim//2, dim//2,  kernel_size=3, stride=1, padding=1, bias=bias)\n            self.project_out2_x = nn.Conv2d(dim, dim//2,  kernel_size=3, stride=1, padding=1, bias=bias)\n            self.project_out2_spec = nn.Conv2d(dim, dim//2,  kernel_size=3, stride=1, padding=1, bias=bias)\n        else:\n            self.project_out1 = nn.Conv2d(dim, dim,  kernel_size=3, stride=1, padding=1, bias=bias)\n            self.project_out2 = nn.Conv2d(dim, dim,  kernel_size=3, stride=1, padding=1, bias=bias)\n\n        # ===========================mask condition===========================\n        self.ResBlock_SFTk = ResBlock_SFT(input_channel = dim,input_mask_dim=1)\n        self.ResBlock_SFTq = ResBlock_SFT(input_channel = dim,input_mask_dim=1)\n        self.out_sft1 = ResBlock_SFT(input_channel = dim,input_mask_dim=1)\n        self.out_sft2 = ResBlock_SFT(input_channel = dim,input_mask_dim=1)\n        self.out_sft3 = ResBlock_SFT(input_channel = dim//2,input_mask_dim=1)\n        self.out_sft4 = ResBlock_SFT(input_channel = dim//2,input_mask_dim=1)\n\n    def forward(self, x_in,spec, material_mask):\n        x = torch.cat([spec,x_in],dim=1)\n        b,c,h,w = x.shape\n\n        qkv = self.qkv_dwconv(self.qkv(x))\n        q,k,v = qkv.chunk(3, dim=1)\n\n        #material semantic prior\n        if self.is_material_mask:\n            q = self.ResBlock_SFTk(q,material_mask)\n            k = self.ResBlock_SFTq(k,material_mask)\n\n        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n\n        q = torch.nn.functional.normalize(q, dim=-1)\n        k = torch.nn.functional.normalize(k, dim=-1)\n\n        attn = (q @ k.transpose(-2, -1)) * self.temperature\n        attn = attn.softmax(dim=-1)\n\n        out = (attn @ v)\n        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n\n        x_out = self.project_out2_x(out) + self.project_out1_x(x_in)\n        spec_out = self.project_out2_spec(out) + self.project_out1_spec(spec)\n        return x_out,spec_out\n\nclass SFTLayer(nn.Module):\n    def __init__(self,dim,input_mask_dim):\n        super(SFTLayer, self).__init__()\n        self.SFT_scale_conv0 = nn.Conv2d(input_mask_dim, dim//2, kernel_size=1, stride=1, padding=0, bias=True) #nn.Conv2d(32, 32, 1)\n        self.SFT_scale_conv1 = nn.Conv2d(dim//2, dim, kernel_size=1, stride=1, padding=0, bias=True)\n        self.SFT_shift_conv0 = nn.Conv2d(input_mask_dim, dim//2, kernel_size=1, stride=1, padding=0, bias=True)\n        self.SFT_shift_conv1 = nn.Conv2d(dim//2, dim, kernel_size=1, stride=1, padding=0, bias=True)\n\n    def forward(self, x,seg):\n        bt, c, h, w = x.shape\n        seg = resize(seg, (h, w), Image.BILINEAR)\n        scale = self.SFT_scale_conv1(F.leaky_relu(self.SFT_scale_conv0(seg), 0.1, inplace=True))\n        shift = self.SFT_shift_conv1(F.leaky_relu(self.SFT_shift_conv0(seg), 0.1, inplace=True))\n        return x * (scale + 1) + shift\nclass ResBlock_SFT(nn.Module):\n    def __init__(self,input_channel,input_mask_dim):\n        super(ResBlock_SFT, self).__init__()\n        self.sft0 = SFTLayer(dim = input_channel,input_mask_dim = input_mask_dim)\n        self.conv0 = nn.Conv2d(input_channel, input_channel, kernel_size=3, stride=1, padding=1, bias=True)\n        self.sft1 = SFTLayer(dim = input_channel,input_mask_dim = input_mask_dim)\n        self.conv1 = nn.Conv2d(input_channel, input_channel, kernel_size=3, stride=1, padding=1, bias=True)\n\n    def forward(self, x,seg):\n        # x[0]: fea; x[1]: cond\n        fea = self.sft0(x,seg)\n        fea = F.relu(self.conv0(fea), inplace=True)\n        fea = self.sft1(fea, seg)\n        fea = self.conv1(fea)\n        return x + fea\n\n\nclass SegExtract(nn.Module):\n    def __init__(self, params, c_in=1):\n        super(SegExtract, self).__init__()\n        self.params = params\n        self.relu = nn.ReLU()\n\n        self.splat1 = nn.Conv2d(c_in, 8, kernel_size=3, stride=1, padding=1, bias=True)#conv_layer(c_in, 8, kernel_size=3, stride=1, padding=1, batch_norm=False)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=(3,3),stride=(2,2),padding=(1,1))#\n        self.splat2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1, bias=True)#conv_layer(8, 16, kernel_size=3, stride=1, padding=1, batch_norm=params['batch_norm'])\n        self.maxpool2 = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))  #\n        #\n        self.splat1_up = nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1, bias=True)#conv_layer(16, 8, kernel_size=3, stride=1, padding=1, batch_norm=params['batch_norm'])\n        self.splat2_up = nn.Conv2d(8, 1, kernel_size=3, stride=1, padding=1, bias=True)#conv_layer(8, 1, kernel_size=3, stride=1, padding=1, batch_norm=params['batch_norm'])\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, x_in):\n        x_in = resize(x_in, (16, 16), Image.BILINEAR)\n\n        x1 = self.splat1(x_in)\n        x1 = self.maxpool1(x1)\n        x1 = self.splat2(x1)\n        x_low1 = self.maxpool2(x1)\n\n        x1 = self.splat1_up(x_low1)\n        x1 = F.interpolate(x1, size=(8,8), mode='bilinear')\n        x1 = self.splat2_up(x1)\n        a = F.interpolate(x1, size=(16,16), mode='bilinear')#self.upsamp2(x1)\n\n        out = 1.0+x_in*(1+a)\n        return out\n\nclass BrightnessAdaptation(nn.Module):\n    def __init__(self, params, c_in=1):\n        super(BrightnessAdaptation, self).__init__()\n        self.params = params\n        self.relu = nn.ReLU()\n\n        self.splat1 = nn.Conv2d(c_in, 8, kernel_size=3, stride=1, padding=1, bias=True)\n        self.splat1_2 = nn.Conv2d(c_in, 8, kernel_size=3, stride=1, padding=1, bias=True)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=(3,3),stride=(2,2),padding=(1,1))#\n        self.splat2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1, bias=True)\n        self.splat2_2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1, bias=True)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))  #\n\n        self.splat1_up = nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1, bias=True)\n        self.splat1_up2 = nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1, bias=True)\n        self.upsamp1 = torch.nn.Upsample(size=(params['output_res'][0]//2,params['output_res'][1]//2), mode='bilinear')\n        self.splat2_up = nn.Conv2d(8, 1, kernel_size=3, stride=1, padding=1, bias=True)\n        self.splat2_up2 = nn.Conv2d(8, 1, kernel_size=3, stride=1, padding=1, bias=True)\n        self.upsamp2 = torch.nn.Upsample(size=(params['output_res'][0],params['output_res'][1]), mode='bilinear')\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, x_in,fullres):\n        x1 = self.splat1(x_in)\n        x1 = self.maxpool1(x1)\n        x1 = self.splat2(x1)\n        x_low1 = self.maxpool2(x1)\n\n        x1 = self.splat1_up(x_low1)\n        x1 = F.interpolate(x1, size=(fullres.size()[-2:][0]//2,fullres.size()[-2:][1]//2), mode='bilinear')\n        x1 = self.splat2_up(x1)\n        a = F.interpolate(x1, size=(fullres.size()[-2:][0],fullres.size()[-2:][1]), mode='bilinear')\n\n        x2 = self.splat1_2(x_in)\n        x2 = self.maxpool1(x2)\n        x2 = self.splat2_2(x2)\n        x_low2 = self.maxpool2(x2)\n\n        x2 = self.splat1_up2(x_low2)\n        x2 = F.interpolate(x2, size=(fullres.size()[-2:][0]//2,fullres.size()[-2:][1]//2), mode='bilinear')\n        x2 = self.splat2_up2(x2)\n        b = F.interpolate(x2, size=(fullres.size()[-2:][0],fullres.size()[-2:][1]), mode='bilinear')\n        out = x_in*(1+a)+b\n        out = torch.clamp(out, 0.01, 1)\n        return out","metadata":{"id":"-55MTTwx-3Ds","executionInfo":{"status":"ok","timestamp":1729141901906,"user_tz":-330,"elapsed":17,"user":{"displayName":"Rohan Phad","userId":"04082063373055029584"}},"execution":{"iopub.status.busy":"2024-10-22T16:12:59.657924Z","iopub.execute_input":"2024-10-22T16:12:59.658230Z","iopub.status.idle":"2024-10-22T16:12:59.706403Z","shell.execute_reply.started":"2024-10-22T16:12:59.658196Z","shell.execute_reply":"2024-10-22T16:12:59.705416Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"markdown","source":"## Models","metadata":{"id":"aiGv7tLA-_1P"}},{"cell_type":"code","source":"import numpy as np\n\nclass FeatureExtract(nn.Module):\n    def __init__(self, params, c_in = 3):\n        super(FeatureExtract, self).__init__()\n        self.params = params\n        self.relu = nn.ReLU()\n        # ===========================attention===========================\n        if self.params['spec']:\n            self.attn1 = SPSA_Attention(dim=8*2, num_heads=1, is_material_mask = self.params['material_mask'], is_spec = self.params['spec'], bias=False)\n            self.attn2 = SPSA_Attention(dim=16*2, num_heads=1,is_material_mask = self.params['material_mask'], is_spec = self.params['spec'], bias=False)\n            self.attn3 = SPSA_Attention(dim=32*2, num_heads=1,is_material_mask = self.params['material_mask'], is_spec = self.params['spec'], bias=False)\n            self.attn4 = SPSA_Attention(dim=64*2, num_heads=1,is_material_mask = self.params['material_mask'], is_spec = self.params['spec'], bias=False)\n        else:\n            self.attn1 = SPSA_Attention(dim=8, num_heads=1, is_material_mask = self.params['material_mask'], is_spec = self.params['spec'], bias=False)\n            self.attn2 = SPSA_Attention(dim=16, num_heads=1,is_material_mask = self.params['material_mask'], is_spec = self.params['spec'], bias=False)\n            self.attn3 = SPSA_Attention(dim=32, num_heads=1,is_material_mask = self.params['material_mask'], is_spec = self.params['spec'], bias=False)\n            self.attn4 = SPSA_Attention(dim=64, num_heads=1,is_material_mask = self.params['material_mask'], is_spec = self.params['spec'], bias=False)\n        # ===========================Fusion===========================\n        self.fusion1 = conv_layer(16, 8,  kernel_size=3, stride=1, padding=1, batch_norm=params['batch_norm'])\n        self.fusion2 = conv_layer(32, 16,  kernel_size=3, stride=1, padding=1, batch_norm=params['batch_norm'])\n        self.fusion3 = conv_layer(64, 32,  kernel_size=3, stride=1, padding=1, batch_norm=params['batch_norm'])\n        self.fusion4 = conv_layer(128, 64,  kernel_size=3, stride=1, padding=1, batch_norm=params['batch_norm'])\n        # ===========================Splat===========================\n        self.splat1 = conv_layer(c_in, 8,  kernel_size=3, stride=2, padding=1, batch_norm=False)\n        self.splat2 = conv_layer(8,    16, kernel_size=3, stride=2, padding=1, batch_norm=params['batch_norm'])\n        self.splat3 = conv_layer(16,   32, kernel_size=3, stride=2, padding=1, batch_norm=params['batch_norm'])\n        self.splat4 = conv_layer(32,   64, kernel_size=3, stride=2, padding=1, batch_norm=params['batch_norm'])\n\n        self.splat1_spec = conv_layer(10,    8, kernel_size=3, stride=2, padding=1, batch_norm=False) #12.18\n        self.splat2_spec = conv_layer(8,    16, kernel_size=3, stride=2, padding=1, batch_norm=params['batch_norm'])\n        self.splat3_spec = conv_layer(16,   32, kernel_size=3, stride=2, padding=1, batch_norm=params['batch_norm'])\n        self.splat4_spec = conv_layer(32,   64, kernel_size=3, stride=2, padding=1, batch_norm=params['batch_norm'])\n        # ===========================Global mine===========================\n        # Conv until 4x4\n        self.global1 = conv_layer(64, 128, kernel_size=3, stride=2, padding=1, batch_norm=params['batch_norm'])\n        self.global2 = conv_layer(128, 256, kernel_size=3, stride=2, padding=1, batch_norm=params['batch_norm'])\n        self.global3 = conv_layer(256, 128, kernel_size=3, stride=2, padding=1, batch_norm=params['batch_norm'])\n        self.global4 = conv_layer(128, 64, kernel_size=3, stride=2, padding=1, batch_norm=params['batch_norm'])\n        self.global5 = conv_layer(64, 64, kernel_size=3, stride=2, padding=1, batch_norm=params['batch_norm'])\n\n        # ===========================Local===========================\n        self.local1 = conv_layer(64, 64, kernel_size=3, padding=1, batch_norm=params['batch_norm'])\n        self.local2 = conv_layer(64, 64, kernel_size=3, padding=1, bias=False, activation=None)\n\n        # ===========================predicton===========================\n        self.pred = conv_layer(64, 96, kernel_size=1, activation=None) # 64 -> 96\n\n    def forward(self, x, spec, material_mask):\n        N = x.shape[0]\n        # ===========================Splat===========================\n        x = self.splat1(x) # N, C=8,  H=128, W=128\n        if self.params['spec']:\n            spec = self.splat1_spec(spec) # N, C=8,  H=128, W=128\n            x,spec = self.attn1(x, spec, material_mask)\n\n        x = self.splat2(x) # N, C=16, H=64,  W=64\n        if self.params['spec']:\n            spec = self.splat2_spec(spec) # N, C=8,  H=128, W=128\n            x,spec = self.attn2(x, spec, material_mask)\n\n        x = self.splat3(x) # N, C=32, H=32,  W=32\n        if self.params['spec']:\n            spec = self.splat3_spec(spec) # N, C=8,  H=128, W=128\n            x,spec = self.attn3(x, spec, material_mask)\n\n        x = self.splat4(x) # N, C=64, H=16,  W=16\n        if self.params['spec']:\n            spec = self.splat4_spec(spec) # N, C=8,  H=128, W=128\n            x,spec = self.attn4(x, spec, material_mask)\n\n        splat_out = x # N, C=64, H=16,  W=16\n        # ===========================Global mine===========================\n        # convs\n        x = self.global1(x)\n        x = self.global2(x)\n        # flatten\n        x = self.global3(x)\n        x = self.global4(x)\n        x = self.global5(x)\n        global_out = x.squeeze(2).squeeze(2)\n        # ===========================Local===========================\n        x = splat_out\n        x = self.local1(x)\n        x = self.local2(x)\n        local_out = x\n        # ===========================Fusion===========================\n        global_out = global_out[:, :, None, None] # N, 64， 1， 1\n        fusion = self.relu(local_out + global_out) # N, C=64, H=16, W=16\n        # ===========================Prediction===========================\n        x = self.pred(fusion) # N, C=96, H=16, W=16\n        x = x.view(N, 12, 8, 16, 16)#16, 16) # N, C=12, D=8, H=16, W=16\n        return x\n\nclass Coefficients(nn.Module):\n    def __init__(self, params, c_in=3):\n        super(Coefficients, self).__init__()\n        self.params = params\n        self.relu = nn.ReLU()\n        # ===========================FeatureExtract===========================\n        self.FeatureExtract0 = FeatureExtract(params,c_in=3)\n        self.FeatureExtract1 = FeatureExtract(params,c_in=3)\n        self.FeatureExtract2 = FeatureExtract(params,c_in=3)\n        self.FeatureExtract3 = FeatureExtract(params,c_in=3)\n        self.FeatureExtract4 = FeatureExtract(params,c_in=3)\n        self.FeatureExtract5 = FeatureExtract(params,c_in=3)\n        self.SegExtract0 = SegExtract(params)\n        self.SegExtract1 = SegExtract(params)\n        self.SegExtract2 = SegExtract(params)\n        self.SegExtract3 = SegExtract(params)\n        self.SegExtract4 = SegExtract(params)\n        self.SegExtract5 = SegExtract(params)\n\n    def forward(self, x, spec, material_mask):\n        #FeatureExtract\n        if self.params['material_mask']:\n            x0 = self.FeatureExtract0(x,spec, material_mask[:,0,:,:].unsqueeze(1))\n            x1 = self.FeatureExtract1(x,spec, material_mask[:,1,:,:].unsqueeze(1))\n            x2 = self.FeatureExtract2(x,spec, material_mask[:,2,:,:].unsqueeze(1))\n            x3 = self.FeatureExtract3(x,spec, material_mask[:,3,:,:].unsqueeze(1))\n            x4 = self.FeatureExtract4(x,spec, material_mask[:,4,:,:].unsqueeze(1))\n            x5 = self.FeatureExtract5(x,spec, material_mask[:,5,:,:].unsqueeze(1))\n            x0_seg = self.SegExtract0(material_mask[:,0,:,:].unsqueeze(1)).unsqueeze(1)\n            x1_seg = self.SegExtract1(material_mask[:,1,:,:].unsqueeze(1)).unsqueeze(1)\n            x2_seg = self.SegExtract2(material_mask[:,2,:,:].unsqueeze(1)).unsqueeze(1)\n            x3_seg = self.SegExtract3(material_mask[:,3,:,:].unsqueeze(1)).unsqueeze(1)\n            x4_seg = self.SegExtract4(material_mask[:,4,:,:].unsqueeze(1)).unsqueeze(1)\n            x5_seg = self.SegExtract5(material_mask[:,5,:,:].unsqueeze(1)).unsqueeze(1)\n            x = x0*x0_seg + x1*x1_seg + x2*x2_seg + x3*x3_seg + x4*x4_seg + x5*x5_seg\n        else:\n            x = self.FeatureExtract0(x,spec, material_mask[:,0,:,:].unsqueeze(1))\n        return x\n\n\nclass Guide(nn.Module):\n    def __init__(self, params, c_in=3):\n        super(Guide, self).__init__()\n        self.params = params\n        # Number of relus/control points for the curve\n        self.nrelus = 16\n        self.c_in = c_in\n        self.M = nn.Parameter(torch.eye(c_in, dtype=torch.float32) + torch.randn(1, dtype=torch.float32) * 1e-4) # (c_in, c_in)\n        self.M_bias = nn.Parameter(torch.zeros(c_in, dtype=torch.float32)) # (c_in,)\n        # The shifts/thresholds in x of relus\n        thresholds = np.linspace(0, 1, self.nrelus, endpoint=False, dtype=np.float32) # (nrelus,)\n        thresholds = torch.tensor(thresholds) # (nrelus,)\n        thresholds = thresholds[None, None, None, :] # (1, 1, 1, nrelus)\n        thresholds = thresholds.repeat(1, 1, c_in, 1) # (1, 1, c_in, nrelus)\n        self.thresholds = nn.Parameter(thresholds) # (1, 1, c_in, nrelus)\n        # The slopes of relus\n        slopes = torch.zeros(1, 1, 1, c_in, self.nrelus, dtype=torch.float32) # (1, 1, 1, c_in, nrelus)\n        slopes[:, :, :, :, 0] = 1.0\n        self.slopes = nn.Parameter(slopes)\n\n        self.relu = nn.ReLU()\n        self.bias = nn.Parameter(torch.tensor(0, dtype=torch.float32))\n\n    def forward(self, x,material_mask,nir):\n        x = x.permute(0, 2, 3, 1) # N, H, W, C=3\n        old_shape = x.shape # (N, H, W, C=3)\n\n        x = torch.matmul(x.reshape(-1, self.c_in), self.M) # N*H*W, C=3\n        x = x + self.M_bias\n        x = x.reshape(old_shape) # N, H, W, C=3\n        x = x.unsqueeze(4) # N, H, W, C=3, 1\n        x = torch.sum(self.slopes * self.relu(x - self.thresholds), dim=4) # N, H, W, C=3\n\n        x = x.permute(0, 3, 1, 2) # N, C=3, H, W\n\n        x = torch.sum(x, dim=1, keepdim=True) / self.c_in # N, C=1, H, W\n        x = x + self.bias # N, C=1, H, W\n        x = torch.clamp(x, 0, 1) # N, C=1, H, W\n        return x\n\n\nclass JDMHDRnetModel(nn.Module):\n    def __init__(self, params):\n        super(JDMHDRnetModel, self).__init__()\n        self.coefficients = Coefficients(params)\n        self.BrightnessAdaptation1 = BrightnessAdaptation(params)\n        self.BrightnessAdaptation2 = BrightnessAdaptation(params)\n        self.BrightnessAdaptation3 = BrightnessAdaptation(params)\n        self.guide = Guide(params)\n\n    def forward(self, lowres, fullres,spec,material_mask,nir):\n        #step1 Brightness Adaptation\n        hue = self.BrightnessAdaptation1(nir,fullres)\n        hue_out = self.BrightnessAdaptation2(nir,fullres)\n        hue_spec = self.BrightnessAdaptation3(nir, fullres)\n        hue_lowres = F.interpolate(hue, size=lowres.size()[-2:],mode='bilinear')\n        hue_spec = F.interpolate(hue_spec, size=lowres.size()[-2:],mode='bilinear')\n        fullres = fullres/hue\n        lowres = lowres/hue_lowres\n        spec = spec/hue_spec\n        # step2 grid coefficient predict\n        grid = self.coefficients(lowres,spec,material_mask)# N, C=12, D=8, H=16, W=16\n        # step3 guide map\n        guide = self.guide(fullres,material_mask,nir) # N, C=1, H, W\n        #step4 slicing\n        sliced = slicing(grid, guide)\n        #step5 generate output\n        output = apply(sliced, fullres)\n        output = output * hue_out\n\n        return output","metadata":{"id":"1ec_rhBC_BNS","executionInfo":{"status":"ok","timestamp":1729141901907,"user_tz":-330,"elapsed":16,"user":{"displayName":"Rohan Phad","userId":"04082063373055029584"}},"execution":{"iopub.status.busy":"2024-10-22T16:12:59.707785Z","iopub.execute_input":"2024-10-22T16:12:59.708129Z","iopub.status.idle":"2024-10-22T16:12:59.762794Z","shell.execute_reply.started":"2024-10-22T16:12:59.708091Z","shell.execute_reply":"2024-10-22T16:12:59.761969Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":"## Datasets","metadata":{"id":"10Fb7MKZ_JI5"}},{"cell_type":"code","source":"import numpy as np\nimport os\nimport torch\nfrom PIL import Image\nfrom skimage import io\nfrom torchvision import transforms\nfrom torchvision.transforms.functional import resize\nfrom torch.utils.data import Dataset\n\nclass BaseDataset(Dataset):\n    def get_tif(self, path, is_jdm_predict):\n        memory_tif = []\n        memory_tif_input = {}\n        memory_tif_output = {}\n        memory_tif_nir = {}\n        memory_spec = {}\n        for file_name in os.listdir(path+'/target'):\n            fname = file_name\n            input = io.imread(os.path.join(path, 'source', fname.split('.')[0] + '.tif'))\n            output = io.imread(os.path.join(path, 'target', fname.split('.')[0] + '.tif'))\n            if is_jdm_predict:\n                nir = io.imread(os.path.join(path, 'nir_jdm', fname.split('.')[0] + '.png'))\n                nir = nir[:, :, 0]\n                nir = (nir // 32 + 1)#1-8\n                nir = nir / 8.0#0.125-1.0\n            else:\n                nir = io.imread(os.path.join(path, 'nir', fname.split('.')[0] + '.tif'))\n            spec = np.load(os.path.join(path, 'spec_npy10band', fname.split('.')[0] + '.npy'), mmap_mode=None,\n                           allow_pickle=False, fix_imports=True, encoding='ASCII')\n\n            memory_tif_input.update({fname.split('.')[0]:input})\n            memory_tif_output.update({fname.split('.')[0]:output})\n            memory_tif_nir.update({fname.split('.')[0]:nir})\n            memory_spec.update({fname.split('.')[0]:spec})\n        memory_tif.append(memory_tif_input)\n        memory_tif.append(memory_tif_output)\n        memory_tif.append(memory_tif_nir)\n        memory_tif.append(memory_spec)\n        return memory_tif\n\n    def load_img_hdr(self, fname,read_memory = False):\n        if read_memory:\n            input = self.memory_tif[0][fname.split('.')[0]]\n            output = self.memory_tif[1][fname.split('.')[0]]\n            nir_ori = self.memory_tif[2][fname.split('.')[0]]\n            spec = self.memory_tif[3][fname.split('.')[0]]\n        else:\n            input = io.imread(os.path.join(self.data_path, 'source', fname.split('.')[0] + '.tif'))\n            output = io.imread(os.path.join(self.data_path, 'target', fname.split('.')[0] + '.tif'))\n            if self.params['jdm_predict']:\n                nir_ori = io.imread(os.path.join(self.data_path, 'nir_jdm', fname.split('.')[0] + '.png'))\n                nir_ori = nir_ori[:, :, 0]\n                nir_ori = (nir_ori // 32 + 1)#1-8\n                nir_ori = nir_ori / 8.0#0.125-1.0\n            else:\n                nir_ori = io.imread(os.path.join(self.data_path, 'nir', fname.split('.')[0] + '.tif'))\n            spec = np.load(os.path.join(self.data_path, 'spec_npy10band', fname.split('.')[0] + '.npy'), mmap_mode=None,\n                           allow_pickle=False, fix_imports=True, encoding='ASCII')\n        if self.params['jdm_predict']:\n            seg = io.imread(os.path.join(self.data_path, 'seg_jdm', fname.split('.')[0] + '.png'))\n        else:\n            seg = io.imread(os.path.join(self.data_path, 'seg', fname.split('.')[0] + '.png'))\n\n        spec = spec.transpose((1, 0, 2))[:,::-1,:]\n        spec = np.ascontiguousarray(spec)\n\n        #############segmentation#################\n        #CLASSES = ('sky','tree','building','trunk','road')\n        #PALETTE =[[19,19,194], [43,139,3], [248,232,109], [78,50,12],[102,102,100]]\n        sky_mask = np.where((seg[:,:,0]==19)&(seg[:,:,1]==19)&(seg[:,:,2]==194),1,0)\n        tree_mask = np.where((seg[:,:,0]==43)&(seg[:,:,1]==139)&(seg[:,:,2]==3),1,0)\n        building_mask = np.where((seg[:,:,0]==248)&(seg[:,:,1]==232)&(seg[:,:,2]==109),1,0)\n        trunk_mask = np.where((seg[:,:,0]==78)&(seg[:,:,1]==50)&(seg[:,:,2]==12),1,0)\n        road_mask = np.where((seg[:,:,0]==102)&(seg[:,:,1]==102)&(seg[:,:,2]==100),1,0)\n        others_mask = np.where((seg[:,:,0]==8)&(seg[:,:,1]==8)&(seg[:,:,2]==8),1,0)\n\n        sky_mask = np.expand_dims(sky_mask,axis=2)\n        tree_mask = np.expand_dims(tree_mask,axis=2)\n        building_mask = np.expand_dims(building_mask,axis=2)\n        trunk_mask = np.expand_dims(trunk_mask,axis=2)\n        road_mask = np.expand_dims(road_mask,axis=2)\n        others_mask = np.expand_dims(others_mask,axis=2)\n\n        material_mask = np.concatenate((sky_mask,tree_mask,building_mask,trunk_mask,\\\n                                         road_mask,others_mask ),axis=2)\n\n        spec = np.asarray(spec, dtype=np.float32)\n        input = np.asarray(input, dtype=np.float32)\n        output = np.asarray(output, dtype=np.float32)\n        material_mask = np.asarray(material_mask, dtype=np.float32)\n        nir_ori = np.asarray(nir_ori, dtype=np.float32)\n\n        #constraint the nir range\n        nir_ori = nir_ori[:,:,np.newaxis]\n        input_max = np.max(input,axis = 2)\n        nir_max = input_max / np.max(input)\n        nir_max = nir_max[:, :, np.newaxis]\n        nir = np.maximum(nir_max,nir_ori)\n\n        input = torch.from_numpy(input.transpose((2, 0, 1)))\n        nir = torch.from_numpy(nir.transpose((2, 0, 1)))\n        output = torch.from_numpy(output.transpose((2, 0, 1)))\n        spec = torch.from_numpy(spec.transpose((2, 0, 1)))\n        material_mask = torch.from_numpy(material_mask.transpose((2, 0, 1)))\n\n        return input, output, spec, material_mask,nir\n\n    def __len__(self):\n        return len(self.input_paths)\n\n\nclass Train_Dataset(BaseDataset):\n    \"\"\"Class for training images.\"\"\"\n\n    def __init__(self, params=None):\n        self.data_path = params['train_data_dir']\n        self.input_paths = get_files(os.path.join(self.data_path, 'source'))\n        self.memory_tif = self.get_tif(self.data_path,params['jdm_predict'])\n        self.input_res = params['input_res']\n        self.output_res = params['output_res']\n\n        self.augment = transforms.Compose([\n            transforms.RandomCrop(self.output_res),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomVerticalFlip(p=0.5),\n        ])\n        self.params = params\n\n    def __getitem__(self, idx):\n\n        fname = self.input_paths[idx].split('/')[-1]\n        if self.params['hdr']:\n            input, output, spec, material_mask,nir = self.load_img_hdr(fname,read_memory=True)\n        # Check dimensions before crop\n        assert input.shape == output.shape\n        assert self.output_res[0] <= input.shape[2]\n        assert self.output_res[1] <= input.shape[1]\n        # Crop\n        inout = torch.cat([input,output,spec,material_mask,nir],dim=0)\n        inout = self.augment(inout)\n\n        full = inout[:3,:,:]\n        low = resize(full, (self.input_res, self.input_res), Image.BILINEAR)\n        output = inout[3:6,:,:]\n\n        spec = inout[6:6+spec.shape[0],:,:]\n        material_mask = inout[6+spec.shape[0]:6+spec.shape[0]+6,:,:]\n        nir = inout[6+spec.shape[0]+6:6+spec.shape[0]+7,:,:]\n        spec_tmp = resize(spec, (self.params['spec_size'], self.params['spec_size']), Image.BILINEAR)\n        spec_low = resize(spec_tmp, (self.input_res, self.input_res), Image.BILINEAR)\n\n        return low, full, output, spec_low,material_mask,nir\n\nclass Eval_Dataset(BaseDataset):\n    \"\"\"Class for validation images.\"\"\"\n\n    def __init__(self, params=None):\n        self.data_path = params['eval_data_dir']\n        self.input_paths = get_files(os.path.join(self.data_path,  'source'))#'input'))\n        # self.memory_tif = self.get_tif(self.data_path)\n        self.input_res = params['input_res']\n        self.params = params\n\n    def __getitem__(self, idx):\n        fname = self.input_paths[idx].split('/')[-1]\n        if self.params['hdr']:\n            full, output, spec, material_mask,nir = self.load_img_hdr(fname,read_memory=False)\n        low = resize(full, (self.input_res, self.input_res), Image.BILINEAR)\n        spec_tmp = resize(spec, (self.params['spec_size'], self.params['spec_size']), Image.BILINEAR)\n        spec_low = resize(spec_tmp, (self.input_res, self.input_res), Image.BILINEAR)\n        return low, full, output, spec_low, material_mask, nir","metadata":{"id":"FLU4ettE_K8m","executionInfo":{"status":"ok","timestamp":1729141902827,"user_tz":-330,"elapsed":933,"user":{"displayName":"Rohan Phad","userId":"04082063373055029584"}},"execution":{"iopub.status.busy":"2024-10-22T16:12:59.764404Z","iopub.execute_input":"2024-10-22T16:12:59.764978Z","iopub.status.idle":"2024-10-22T16:12:59.811471Z","shell.execute_reply.started":"2024-10-22T16:12:59.764934Z","shell.execute_reply":"2024-10-22T16:12:59.810486Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{"id":"BlCDlXeM_RFu"}},{"cell_type":"code","source":"import numpy as np\nimport os\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom argparse import ArgumentParser\nfrom torch.optim import Adam, lr_scheduler\nfrom torchvision.transforms.functional import vflip, hflip\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import save_image\n\n\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n\n\ndef train(params, train_loader, valid_loader, model, ep, device):\n    \n#     if torch.cuda.device_count() > 1:\n#         print(f\"Using {torch.cuda.device_count()} GPUs!\")\n#         # Wrap the entire model in DataParallel\n#         model = nn.DataParallel(model)\n    \n    model = model.to(device)\n    \n    # Optimization\n    optimizer = Adam(model.parameters(), params['learning_rate'], weight_decay=1e-8)\n    if not params['material_mask']:\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n        patience=params['epochs']*3, factor=0.5, verbose=True)\n    # Loss function\n    criterion = nn.MSELoss()\n    # Training\n    train_loss_meter = AvgMeter()\n    train_psnr_meter = AvgMeter()\n    early_stopping_counter = 100\n    stats = {'train_loss': [],\n             'train_psnr': [],\n             'valid_psnr': []}\n    iteration = (ep+1)*160 / params['batch_size']\n    # print(iteration)\n    old_time = time.time()\n\n    for epoch in range(ep+1,params['epochs']):\n        for batch_idx, (low, full, target, spec, material_mask,nir) in enumerate(train_loader):\n            iteration += 1\n            model.train()\n\n            low = low.to(device)\n            full = full.to(device)\n            target = target.to(device)\n            spec = spec.to(device)\n            material_mask = material_mask.to(device)\n            nir = nir.to(device)\n\n            if params['debugsave']:\n                full_image = full[0, :, :, :]\n                full_save =  (full_image -torch.min(full_image))/ (torch.max(full_image)- torch.min(full_image)) #* 255\n                save_image(full_save, os.path.join(params['eval_out'], str(batch_idx)+'_inputfull.tif'))\n                save_image(nir[0,:,:,:], os.path.join(params['eval_out'], str(batch_idx)+'_nir.tif'))\n                ori_imgae = nir[0, :, :, :]*full[0,:,:,:]\n                ori_save = (ori_imgae - torch.min(ori_imgae)) / (torch.max(ori_imgae) - torch.min(ori_imgae)) #* 255\n                save_image(ori_save, os.path.join(params['eval_out'], str(batch_idx) + '_ori.tif'))\n                save_image(target[0, :, :, :] / nir[0,:,:,:] /255, os.path.join(params['eval_out'], str(batch_idx) + '_targetR.png'))\n\n                save_image(target[0,:,:,:]/255, os.path.join(params['eval_out'], str(batch_idx)+'_target.png'))\n                save_image(spec[0,0,:,:]/65535, os.path.join(params['eval_out'], str(batch_idx)+'_spec.png'))\n\n                # save_image(full[0,:,:,:]/65535*255, os.path.join(params['eval_out'], str(batch_idx)+'_full.tif'))\n                # save_image(target[0,:,:,:]/255, os.path.join(params['eval_out'], str(batch_idx)+'_target.png'))\n                # save_image(spec[0,0,:,:]/65535, os.path.join(params['eval_out'], str(batch_idx)+'_spec.png'))\n                save_image(material_mask[0,0,:,:], os.path.join(params['eval_out'], str(batch_idx)+'_material_sky_mask.png'))\n                save_image(material_mask[0,1,:,:], os.path.join(params['eval_out'], str(batch_idx)+'_material_tree_mask.png'))\n                save_image(material_mask[0,2,:,:], os.path.join(params['eval_out'], str(batch_idx)+'_material_building_mask.png'))\n                save_image(material_mask[0,3,:,:], os.path.join(params['eval_out'], str(batch_idx)+'_material_trunk_mask.png'))\n                save_image(material_mask[0,4,:,:], os.path.join(params['eval_out'], str(batch_idx)+'_material_road_mask.png'))\n                save_image(material_mask[0,5,:,:], os.path.join(params['eval_out'], str(batch_idx)+'_material_others_mask.png'))\n            # Normalize to [0, 1] on GPU\n            if params['hdr']:\n                low = torch.div(low, 65535.0)\n                full = torch.div(full, 65535.0)\n                spec = torch.div(spec, 65535.0)\n            else:\n                low = torch.div(low, 255.0)\n                full = torch.div(full, 255.0)\n            target = torch.div(target, 255.0)\n\n            output = model(low, full, spec, material_mask,nir)\n\n            loss = criterion(output, target)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            if not params['material_mask']:\n                scheduler.step(loss)\n\n            if iteration % params['summary_interval'] == 0:\n                train_loss_meter.update(loss.item())\n                train_psnr = psnr(output, target).item()\n                train_psnr_meter.update(train_psnr)\n                new_time = time.time()\n                print('[%d/%d] Iteration: %d | Loss: %.4f | PSNR: %.4f | lr: %.8f | Time: %.2fs' %\n                        (epoch+1, params['epochs'], iteration, loss, train_psnr, optimizer.param_groups[0]['lr'], new_time-old_time))\n                old_time = new_time\n\n            if iteration % params['ckpt_interval'] == 0:\n                stats['train_loss'].append(train_loss_meter.avg)\n                train_loss_meter.reset()\n                stats['train_psnr'].append(train_psnr_meter.avg)\n                train_psnr_meter.reset()\n                valid_psnr = eval(params, valid_loader, model, device, epoch)\n                stats['valid_psnr'].append(valid_psnr)\n                ckpt_fname = \"epoch_\" + str(epoch)+'_iter_' + str(iteration) + \".pt\"\n                save_model_stats(model, params, ckpt_fname, stats)\n                if(valid_psnr >= 30):\n                    return\n\ndef eval(params, valid_loader, model, device,epoch):\n    model.eval()\n    psnr_meter = AvgMeter()\n    with torch.no_grad():\n        for batch_idx, (low, full, target, spec, material_mask,nir) in enumerate(valid_loader):\n            low = low.to(device)\n            full = full.to(device)\n            target = target.to(device)\n            spec = spec.to(device)\n            nir = nir.to(device)\n            material_mask = material_mask.to(device)\n\n            # Normalize to [0, 1] on GPU\n            if params['hdr']:\n                low =  torch.div(low, 65535.0)\n                full = torch.div(full, 65535.0)\n                spec = torch.div(spec, 65535.0)\n            else:\n                low = torch.div(low, 255.0)\n                full = torch.div(full, 255.0)\n            target = torch.div(target, 255.0)\n\n\n            output= model(low, full, spec, material_mask,nir)\n\n            # output = output * nir\n\n            # save_image(output, os.path.join(params['eval_out'], 'epoch'+str(epoch)+'_'+str(batch_idx)+'.png'))\n\n\n            eval_psnr = psnr(output, target).item()\n            print(str(batch_idx)+'.png',eval_psnr)\n            psnr_meter.update(eval_psnr)\n\n    print (\"Validation PSNR: \", psnr_meter.avg)\n\n    return psnr_meter.avg\n\n\ndef train_main(params, first_time=True):\n    # Random seeds\n    seed = 0\n    torch.backends.cudnn.deterministic = True # False\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\n    print_params(params)\n\n    os.makedirs(params['ckpt_dir'], exist_ok=True)\n    os.makedirs(params['stats_dir'], exist_ok=True)\n    os.makedirs(params['eval_out'], exist_ok=True)\n\n    train_dataset = Train_Dataset(params)\n    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n\n    valid_dataset = Eval_Dataset(params)\n    valid_loader = DataLoader(valid_dataset, batch_size=1)\n\n    model = JDMHDRnetModel(params)\n    prev_epochs = load_train_ckpt(model, params['ckpt_dir'])\n    if first_time:\n        prev_epochs = -1\n    print(\"prev_epochs \", prev_epochs)\n    if params['cuda']:\n        device = torch.device(\"cuda\")\n    else:\n        device = torch.device(\"cpu\")\n    model.to(device)\n\n    train(params, train_loader, valid_loader, model, prev_epochs, device)","metadata":{"id":"Voqxdq2q_SfC","executionInfo":{"status":"ok","timestamp":1729141902828,"user_tz":-330,"elapsed":16,"user":{"displayName":"Rohan Phad","userId":"04082063373055029584"}},"execution":{"iopub.status.busy":"2024-10-22T16:12:59.812950Z","iopub.execute_input":"2024-10-22T16:12:59.813305Z","iopub.status.idle":"2024-10-22T16:12:59.852321Z","shell.execute_reply.started":"2024-10-22T16:12:59.813269Z","shell.execute_reply":"2024-10-22T16:12:59.851502Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"markdown","source":"## Test","metadata":{"id":"UD5RUGir_qqL"}},{"cell_type":"code","source":"import numpy as np\nimport os\nimport torch\nfrom argparse import ArgumentParser\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import save_image\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n\ndef eval(params, valid_loader, model, device,epoch):\n    model.eval()\n    psnr_meter = AvgMeter()\n    with torch.no_grad():\n        for batch_idx, (low, full, target, spec, material_mask,nir) in enumerate(valid_loader):\n            low = low.to(device)\n            full = full.to(device)\n            target = target.to(device)\n            spec = spec.to(device)\n            nir = nir.to(device)\n            material_mask = material_mask.to(device)\n\n            # Normalize to [0, 1] on GPU\n            if params['hdr']:\n                low =  torch.div(low, 65535.0)\n                full = torch.div(full, 65535.0)\n                spec = torch.div(spec, 65535.0)\n            else:\n                low = torch.div(low, 255.0)\n                full = torch.div(full, 255.0)\n            target = torch.div(target, 255.0)\n\n            output= model(low, full, spec, material_mask,nir)\n            mul = 1;\n            if batch_idx >=10 and batch_idx<100:\n                mul=2\n            if batch_idx >=100 and batch_idx<1000:\n                mul=3\n            save_image(output, os.path.join(params['eval_out'], '0'*(4-mul) + str(batch_idx)+'.png'))\n\n            eval_psnr = psnr(output, target).item()\n            print(str(batch_idx)+'.png',eval_psnr)\n            psnr_meter.update(eval_psnr)\n\n    print (\"Validation PSNR: \", psnr_meter.avg)\n\n    return psnr_meter.avg\n\n\n\ndef test(params):\n    # Random seeds\n    seed = 0\n    torch.backends.cudnn.deterministic = True # False\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\n    print_params(params)\n\n    os.makedirs(params['ckpt_dir'], exist_ok=True)\n    os.makedirs(params['stats_dir'], exist_ok=True)\n    os.makedirs(params['eval_out'], exist_ok=True)\n\n    valid_dataset = Eval_Dataset(params)\n    valid_loader = DataLoader(valid_dataset, batch_size=1)\n\n    model = JDMHDRnetModel(params)\n    load_train_ckpt(model, params['ckpt_dir'])\n    if params['cuda']:\n        device = torch.device(\"cuda\")\n    else:\n        device = torch.device(\"cpu\")\n    model.to(device)\n\n    valid_psnr = eval(params, valid_loader, model, device, epoch=params['epochs'])","metadata":{"id":"EHNmE1uE_sQP","executionInfo":{"status":"ok","timestamp":1729141902829,"user_tz":-330,"elapsed":15,"user":{"displayName":"Rohan Phad","userId":"04082063373055029584"}},"execution":{"iopub.status.busy":"2024-10-22T16:12:59.853811Z","iopub.execute_input":"2024-10-22T16:12:59.854183Z","iopub.status.idle":"2024-10-22T16:12:59.870539Z","shell.execute_reply.started":"2024-10-22T16:12:59.854108Z","shell.execute_reply":"2024-10-22T16:12:59.869693Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"def parse_params(jdm_predict=False):\n  params = {\n      \"cuda\": True,\n      \"ckpt_interval\": 600,\n      \"ckpt_dir\": \"./ckptsJdm\",  # Make this different for Ideal and predicted case\n      \"stats_dir\": \"./stats_jdm\", # Make this different for Ideal and predicted case\n      \"epochs\": 6000,\n      \"learning_rate\": 1e-4,\n      \"summary_interval\": 10,\n      \"batch_size\": 16,\n      \"train_data_dir\": '/kaggle/input/mobile-spec/Mobile-Spec/train',  # Change this to your path\n      \"eval_data_dir\": '/kaggle/input/mobile-spec/Mobile-Spec/eval',    # Change this to your path\n      \"eval_out\": \"./outputs_jdm\",\n      \"hdr\": True,\n      \"jdm_predict\": jdm_predict,\n      \"batch_norm\": False,\n      \"input_res\": 256,\n      \"output_res\": (512, 512),\n      \"spec_size\": 16,\n      \"spec\": True,\n      \"material_mask\": True,\n      \"debugsave\": False,\n  }\n  return params\n","metadata":{"id":"JcuiQ7CcoO-r","executionInfo":{"status":"ok","timestamp":1729141902829,"user_tz":-330,"elapsed":13,"user":{"displayName":"Rohan Phad","userId":"04082063373055029584"}},"execution":{"iopub.status.busy":"2024-10-22T16:12:59.871636Z","iopub.execute_input":"2024-10-22T16:12:59.871926Z","iopub.status.idle":"2024-10-22T16:12:59.885926Z","shell.execute_reply.started":"2024-10-22T16:12:59.871895Z","shell.execute_reply":"2024-10-22T16:12:59.884999Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"# def delete_all_files(directory_path):\n#     # Loop through all files in the directory\n#     for filename in os.listdir(directory_path):\n#         file_path = os.path.join(directory_path, filename)\n        \n#         # Check if it's a file before deleting\n#         if os.path.isfile(file_path):\n#             os.remove(file_path)\n#             print(f\"Deleted: {file_path}\")\n#         else:\n#             print(f\"Skipped (not a file): {file_path}\")\n\n# # Example usage\n# directory_path = '/kaggle/working/outputs_jdm'  # Replace with your directory path\n# delete_all_files(directory_path)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:12:59.889976Z","iopub.execute_input":"2024-10-22T16:12:59.890311Z","iopub.status.idle":"2024-10-22T16:12:59.896468Z","shell.execute_reply.started":"2024-10-22T16:12:59.890279Z","shell.execute_reply":"2024-10-22T16:12:59.895436Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"markdown","source":"### Predicted priors training","metadata":{"id":"tXbu0ENlB631"}},{"cell_type":"code","source":"params = parse_params(True)","metadata":{"id":"UjG7H_typbj5","executionInfo":{"status":"ok","timestamp":1729141905059,"user_tz":-330,"elapsed":10,"user":{"displayName":"Rohan Phad","userId":"04082063373055029584"}},"execution":{"iopub.status.busy":"2024-10-22T16:12:59.897696Z","iopub.execute_input":"2024-10-22T16:12:59.898537Z","iopub.status.idle":"2024-10-22T16:12:59.909465Z","shell.execute_reply.started":"2024-10-22T16:12:59.898492Z","shell.execute_reply":"2024-10-22T16:12:59.908704Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"train_main(params, True) # True if training for first time, False if there's already saved ckpt.","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:12:59.910600Z","iopub.execute_input":"2024-10-22T16:12:59.910872Z","iopub.status.idle":"2024-10-22T16:12:59.920713Z","shell.execute_reply.started":"2024-10-22T16:12:59.910842Z","shell.execute_reply":"2024-10-22T16:12:59.919801Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"test(params)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:12:59.932953Z","iopub.execute_input":"2024-10-22T16:12:59.933447Z","iopub.status.idle":"2024-10-22T16:13:43.581018Z","shell.execute_reply.started":"2024-10-22T16:12:59.933404Z","shell.execute_reply":"2024-10-22T16:13:43.579976Z"},"trusted":true},"execution_count":123,"outputs":[{"name":"stdout","text":"Training parameters: \n  cuda = True\n  ckpt_interval = 600\n  ckpt_dir = ./ckptsJdm\n  stats_dir = ./stats_jdm\n  epochs = 6000\n  learning_rate = 0.0001\n  summary_interval = 10\n  batch_size = 16\n  train_data_dir = /kaggle/input/mobile-spec/Mobile-Spec/train\n  eval_data_dir = /kaggle/input/mobile-spec/Mobile-Spec/eval\n  eval_out = ./outputs_jdm\n  hdr = True\n  jdm_predict = True\n  batch_norm = False\n  input_res = 256\n  output_res = (512, 512)\n  spec_size = 16\n  spec = True\n  material_mask = True\n  debugsave = False\n\nepochs  2879\nLoading: ./ckptsJdm/epoch_2879_iter_28800.0_jdm.pt\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_29/916897755.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(ckpt_path)\n","output_type":"stream"},{"name":"stdout","text":"0.png 27.73189353942871\n1.png 31.836633682250977\n2.png 30.904296875\n3.png 30.982990264892578\n4.png 32.085533142089844\n5.png 28.85420036315918\n6.png 28.37204360961914\n7.png 34.19792556762695\n8.png 23.507020950317383\n9.png 29.33184051513672\n10.png 30.91350746154785\n11.png 29.12506103515625\n12.png 30.225200653076172\n13.png 25.5830078125\n14.png 24.38079833984375\n15.png 29.393585205078125\n16.png 32.894622802734375\n17.png 25.306655883789062\n18.png 30.861827850341797\n19.png 26.626699447631836\n20.png 25.449974060058594\n21.png 27.393892288208008\n22.png 24.431962966918945\n23.png 28.57193946838379\n24.png 28.185894012451172\n25.png 27.369670867919922\n26.png 28.384002685546875\n27.png 34.350948333740234\n28.png 26.63776397705078\n29.png 26.087993621826172\n30.png 33.71211242675781\n31.png 33.79047393798828\n32.png 27.36471176147461\n33.png 30.91245460510254\n34.png 29.494464874267578\n35.png 26.7781925201416\n36.png 31.977962493896484\n37.png 30.213830947875977\n38.png 26.34627914428711\n39.png 29.910125732421875\nValidation PSNR:  29.011999893188477\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Ideal Priors training","metadata":{}},{"cell_type":"code","source":"params = parse_params(False)\nparams['ckpt_dir'] = './ckptsIdeal'\nparams['eval_out'] = './outputs_ideal'\nparams['stats_dir'] = './stats_ideal'","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:13:43.582244Z","iopub.execute_input":"2024-10-22T16:13:43.582551Z","iopub.status.idle":"2024-10-22T16:13:43.587126Z","shell.execute_reply.started":"2024-10-22T16:13:43.582518Z","shell.execute_reply":"2024-10-22T16:13:43.586227Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"train_main(params, True)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:13:43.588055Z","iopub.execute_input":"2024-10-22T16:13:43.588490Z","iopub.status.idle":"2024-10-22T16:13:43.597022Z","shell.execute_reply.started":"2024-10-22T16:13:43.588450Z","shell.execute_reply":"2024-10-22T16:13:43.596234Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"test(params)","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:13:43.598160Z","iopub.execute_input":"2024-10-22T16:13:43.598423Z","iopub.status.idle":"2024-10-22T16:14:27.583426Z","shell.execute_reply.started":"2024-10-22T16:13:43.598394Z","shell.execute_reply":"2024-10-22T16:14:27.582441Z"},"trusted":true},"execution_count":126,"outputs":[{"name":"stdout","text":"Training parameters: \n  cuda = True\n  ckpt_interval = 600\n  ckpt_dir = ./ckptsIdeal\n  stats_dir = ./stats_ideal\n  epochs = 6000\n  learning_rate = 0.0001\n  summary_interval = 10\n  batch_size = 16\n  train_data_dir = /kaggle/input/mobile-spec/Mobile-Spec/train\n  eval_data_dir = /kaggle/input/mobile-spec/Mobile-Spec/eval\n  eval_out = ./outputs_ideal\n  hdr = True\n  jdm_predict = False\n  batch_norm = False\n  input_res = 256\n  output_res = (512, 512)\n  spec_size = 16\n  spec = True\n  material_mask = True\n  debugsave = False\n\nepochs  2879\nLoading: ./ckptsIdeal/epoch_2879_iter_28800.0.pt\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_29/916897755.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(ckpt_path)\n","output_type":"stream"},{"name":"stdout","text":"0.png 26.598548889160156\n1.png 30.439321517944336\n2.png 30.95160484313965\n3.png 30.279176712036133\n4.png 33.38441467285156\n5.png 33.31548309326172\n6.png 25.933650970458984\n7.png 31.057437896728516\n8.png 23.274181365966797\n9.png 29.386669158935547\n10.png 29.240585327148438\n11.png 29.924623489379883\n12.png 30.9648494720459\n13.png 27.624462127685547\n14.png 24.656953811645508\n15.png 26.830293655395508\n16.png 30.957992553710938\n17.png 26.00284767150879\n18.png 30.350610733032227\n19.png 30.518394470214844\n20.png 26.626794815063477\n21.png 26.125925064086914\n22.png 23.82185935974121\n23.png 28.421512603759766\n24.png 25.644432067871094\n25.png 26.469913482666016\n26.png 33.29463195800781\n27.png 31.12220573425293\n28.png 25.500797271728516\n29.png 28.88542938232422\n30.png 31.094627380371094\n31.png 34.26521682739258\n32.png 26.461318969726562\n33.png 33.399662017822266\n34.png 30.7695369720459\n35.png 29.26552963256836\n36.png 31.84986114501953\n37.png 29.237306594848633\n38.png 29.804649353027344\n39.png 25.787267684936523\nValidation PSNR:  28.988514518737794\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Calculating ssim","metadata":{}},{"cell_type":"code","source":"from skimage import io, img_as_float\nfrom skimage.metrics import structural_similarity as ssim\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:14:27.635554Z","iopub.execute_input":"2024-10-22T16:14:27.635896Z","iopub.status.idle":"2024-10-22T16:14:27.645830Z","shell.execute_reply.started":"2024-10-22T16:14:27.635856Z","shell.execute_reply":"2024-10-22T16:14:27.644777Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"\n# Function to load images from two directories and calculate SSIM for each pair\ndef load_and_calculate_ssim(dir1, dir2):\n    ssim_values = []\n#     E_values = []\n\n    # Get sorted list of image filenames from both directories\n    files1 = sorted([f for f in os.listdir(dir1) if f.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tif'))])\n    files2 = sorted([f for f in os.listdir(dir2) if f.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tif'))])\n\n    # Ensure both directories have the same number of images\n    if len(files1) != len(files2):\n        print(len(files1))\n        print(len(files2))\n        print(\"The number of images in both directories must be the same.\")\n        return\n\n    # Loop through the files and calculate SSIM for each pair\n    for file1, file2 in zip(files1, files2):\n        # Load images from both directories\n        image1_path = os.path.join(dir1, file1)\n        image2_path = os.path.join(dir2, file2)\n        image1 = cv2.imread(image1_path)\n        image2 = cv2.imread(image2_path)\n\n        if image1 is not None and image2 is not None:\n            # Calculate SSIM for the current pair of images\n            ssim_value = calculate_ssim(image1, image2)\n#             E_value = calculate_delta_e_tiff(image1_path, image2_path)\n#             print(ssim_value)\n            ssim_values.append(ssim_value)\n#             E_values.append(E_value)\n        else:\n            print(f\"Error loading images {file1} or {file2}\")\n    \n    return ssim_values\n\n# Example usage\ndir1 = '/kaggle/working/outputs_jdm'  # Replace with your directory path\ndir2 = '/kaggle/input/mobile-spec/Mobile-Spec/eval/target'  # Replace with your directory path\n\nssim_results = load_and_calculate_ssim(dir1, dir2)\n# print(ssim_results)\n# Print SSIM results\nssim = np.mean(ssim_results)\nprint(f'SSIM : {ssim}')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-22T16:29:37.619545Z","iopub.execute_input":"2024-10-22T16:29:37.619924Z","iopub.status.idle":"2024-10-22T16:29:43.297301Z","shell.execute_reply.started":"2024-10-22T16:29:37.619887Z","shell.execute_reply":"2024-10-22T16:29:43.296389Z"},"trusted":true},"execution_count":150,"outputs":[{"name":"stdout","text":"SSIM : 0.9629230426738514\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}